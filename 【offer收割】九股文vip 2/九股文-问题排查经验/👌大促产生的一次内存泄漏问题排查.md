[![14、多对象导致的内存泄漏.mp4 (16.55MB)](https://gw.alipayobjects.com/mdn/prod_resou/afts/img/A*NNs6TKOR3isAAAAAAAAAAABkARQnAQ)](https://www.yuque.com/docs/176645993?_lake_card=%7B%22status%22%3A%22done%22%2C%22name%22%3A%2214%E3%80%81%E5%A4%9A%E5%AF%B9%E8%B1%A1%E5%AF%BC%E8%87%B4%E7%9A%84%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F.mp4%22%2C%22size%22%3A17358571%2C%22taskId%22%3A%22u32f8a7e0-67bb-40eb-baf6-d6a1b0827df%22%2C%22taskType%22%3A%22upload%22%2C%22url%22%3Anull%2C%22cover%22%3Anull%2C%22videoId%22%3A%22inputs%2Fprod%2Fyuque%2F2024%2F29413969%2Fmp4%2F1719845604649-b66fac3e-9bb4-46af-92e8-38f08c542d87.mp4%22%2C%22download%22%3Afalse%2C%22__spacing%22%3A%22both%22%2C%22id%22%3A%22scSVU%22%2C%22margin%22%3A%7B%22top%22%3Atrue%2C%22bottom%22%3Atrue%7D%2C%22card%22%3A%22video%22%7D#scSVU)这道题是在大促的时候产生的一次内存泄漏问题，给大家进行一波讲解和排查。这样当面试官问你内存泄漏问题有没有排查过的时候，你就可以咔咔乱杀了！<br />本排查案例是通用的！大家消化完成，直接套到自己项目即可！<br />本题配备了视频助于理解。
## 背景
马上临近大促（你们可以说某天收到了一个内存占用过高的一个报警），对于一些机器的各种报警特别敏感，晚上收到其中一个应用几条内存占用过高的报警，打开监控界面及历史消息发现该应用最近频繁内存报警，采用命令进行fullGC也并不能回收内存，目前采用重启应用解决该报警问题，为了从根本上解决问题，所以决定查一下具体原因。
## 现象：
经观察和分析，发现该应用有以下主要症状：
### 1、内存趋势
内存在使用高峰期（8:00-20:00）增长较快，在低峰期增长较慢，半夜睡觉无什么增长。
### 2、线程数趋势
应用JVM监控中的线程数持续增长，同时，使用高峰期（8:00-20:00）增长较快，在低峰期增长较慢。<br />由以上两个症状，基本可以确定该应用内存持续上升（内存泄露），是由线程不断创建导致，那具体是哪个线程呢？接下来，我们将该应用的dump文件导出。
## 解决思路
### 1、选择工具，查找线程。
使用MemoryAnalyzerlTool进行分析，将dump文件导入工具，查看线程概况，并查看线程列表，发现前缀为Oss-Thread的线程很多，于是采用Oss进行过滤，过滤出1200多个线程。 
### 2、找到创建线程的对象
那么该线程是由那个对象创建的呢？于是点开线程详情，我们可以看到，该线程是由oss lib包中的OSS类对应的实例创建。
### 3、结合源码分析线程创建原因
在IDE中打开该应用的代码，并打开OSS的源码，果然是该类的问题，该类在执行init()的时候，会创建一个定时任务线程，并会每天执行一次，
### 4、深入查询线程创建原因
继续查询该方法什么时候调用？由于我们是采用反编译拿到的代码，并没有源码，所以我们目前只能通过分析，去定位该方法被那里调用，首先，该类位于oss的lib中，我们用oss是为了上传文件，上传文件之前我们都会有对应的ossService实例，那大概率和该对应有关系，所以沿着这个思路，找到ossService的构造方法。<br />发现构造过程确实会创建这个类。
### 5、查出元凶
果然在该构造中，有oss类实例化的动作，以及执行init()的动作，至此，基本已经确定是ossService在不断创建导致，那通常我们都是使用spring管理的单例对象，为什么会不断创建ossService对应呢？所以全局检索 new ossService，一看吓一跳。
### 6、分析与优化
#### 分析：
哇，简直了，有同学上传的时候竟然是每上传一次就创建一个ossService实例，并且涉及到好多场景和功能，且没有进行destroy()，就会导致ossService实例越来越多，oss-Thread线程也越来越多，引发内存泄露。
#### 优化：
于是，将所有功能优化，采用spring 管理的ossService实例，优化完毕，上线持续观察，内存和线程基本维持较为平稳。
## 总结
至此，内存泄露问题终于解决，在解决的过程中，还是发现很多我们日常编写代码的不良习惯，比如，拷贝代码，没有做深入思考，直接粘过来使用，至于是否会带来风险以及原有代码是否合理，并没有做过多的评估，导致问题不断放大，最后可能引发线上事故。<br />ps：<br />为什么之前一直没有发现这个问题？<br />因为该内存的增长过程很缓慢，可能需要发布一周左右才能发现，而之前该应用发布频繁，内存还没有爆的时候，应用就被发布了，导致该问题没有被识别到，而最近一段时间，该应用没有发布，所以，问题凸显出来。
