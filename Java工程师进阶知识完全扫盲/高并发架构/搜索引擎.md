
# ES 的分布式架构原理是什么

1. 分布式搜索引擎=>底层基于lucene=>多台机器上启动多个ES进程实例=>组成ES集群
2. 存储数据的基本单位是索引 类比数据库可以这样
   index=>表
   document=>行
   field=>列
   mapping=>表结构定义
   node=>每一个服务器
   shard replica=>数据分片与备份
3. index=>拆分多个shard=>支持横向扩展=>提高性能(吞吐量)
4. shard=>由primary shard 和 多个replica shard组成=>primary shard写入数据=>同步到其他replica shard
5. 多个node=>选举一个master节点=>管理工作(维护索引元数据 切换primary shard和 replica shard)
   master节点宕机=>重新选举一个节点为master节点
6. 非master节点宕机了=>此节点上的primary shard失效了=>master节点会让对应的replica shard切换为primary shard
   宕机的节点修复了=>原来的primary shard=>成为replica shard

# ES 写入数据的工作原理是什么

## es写数据过程

客户端选择一个node(协调节点)=>节点对document进行路由=>请求转发给对应的node(primary shard)=>处理请求然后同步到 replica node=>协调节点返回响应结果给客户端。

## es读数据过程

客户端发送请求到任意一个node(协调节点)=>对 doc id 进行哈希路由=>请求转发到对应的node=>使用随机轮询算法在primary和replica shard中随机选择一个=>
接受请求的node返回document给协调节点=>协调节点返回document给客户端

## es搜索数据过程

客户端发送请求到协调节点=>转发到所有shard(primary 或者 replica都可)=>每个shard将自己的搜索结果( doc id)返回给协调节点=>协调节点进行数据的合并，排序，分页=>协调节点根据 doc id 去各个节点拉取实际的document数据=>返回给客户端

## 写数据底层原理

数据写入buffer=>每隔一秒refresh到 os cache(数据能被搜索到)=>每隔5s将数据写入translog文件(最多5s数据丢失)=>translog大到一定程度或者每隔30min=>

触发commit操作将缓冲区的数据flush到segment file磁盘文件中(倒排索引已建立)

## 删除/更新数据底层原理

**删除**

commit的时候生成一个.del文件=>将某个doc标识为deleted状态=>搜索的时候根据.del文件判断这个doc是否被删除

**更新**

原来的doc标识为deleted=>新写入一条数据

**物理上的删除**
定期执行merge将多个segment合并成一个=>标识为deleted的doc给物理删除掉

## 底层lucene

jar包，封装好各种建立倒排索引的算法代码，lucene将已有的数据建立索引，在本地磁盘上组织它的数据结构。

## 倒排索引

倒排索引就是关键词到文档ID的映射。

1. 所有词项对应一个或多个文档
2. 词项根据字典顺序升序排列

# ES 在数据量很大的情况下（数十亿级别）如何提高查询效率啊？

## 文件系统缓存

1. filesystem cache 足够大尽可能容纳所有的idx segment file(索引数据文件)
2. es中存储数据，尽量只存需要用来检索的字段
3. es+mysql/hbase（es中检索得到数据条目的id,再通过id去mysql中查询所有信息）

## 数据预热

对于比较热的，经常会有人访问的数据，做一个专门的**缓存预热子系统**。

系统的功能是对热数据每隔一段时间，提前访问一下，让数据进入filesystem cache 。

## 冷热分离

冷数据写入一个索引，热数据写入另一个索引中，确保热数据在被预热之后，尽量让他们留在filesystem os cache里，不让冷数据给冲刷掉。

## document模型设计

es中避免关联查询，而是将关联好的数据直接放入es。

## 分页性能优化

1. 不允许深度分页
2. 滚动分页(scroll api)

# ES 生产集群的部署架构是什么？

1. es生产集群我们部署了5台机器，每台机器是6核64G，集群总内存是320G。
2. 集群日增量数据大概2000万条，日新增占存储空间500MB，每月是6亿条数据，新增占用空间15G，运行几个月，数据总量大概100G左右。
3. 线上有五个索引，每个索引的数据量大概是20G，每个索引分配的是8个shard，比默认的5个shard多了3个shard。


