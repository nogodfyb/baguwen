# 高并发架构

## 消息队列

### 为什么使用消息队列

#### 使用场景

##### 解耦

场景：

A系统调用BCD三个系统的接口，如果B系统不需要接收数据了呢？如果又有新系统E需要这份数据呢。并且需要考虑BCDE系统挂了怎么办，是用重发解决还是把消息存起来?

**使用MQ进行解耦**

A系统将消息发送到MQ，BCDE系统谁需要这份消息就去订阅进行消费，这样就进行解耦了。

**总结**

通过一个MQ，PUB/SUB 发布订阅消息这一个模型，A系统就和其它系统进行解耦了。

##### 异步

场景：

A系统接受请求->A系统写库->B系统写库->C系统写库->D系统写库

总共耗时=A系统10ms+B系统200ms+C系统300ms+D系统300ms=810ms

如何优化这个请求耗时降到200ms以内？

**使用MQ进行削峰**

A系统接受请求->向MQ中发送三条消息以供BCD系统进行消费

总共耗时=A系统原本耗时10ms+三条消息入队列耗时10ms=20ms

##### 削峰

场景：

系统在高峰期涌入过量请求，这些请求会涌入MySQL,导致MySQL抗不住然后系统崩溃。

假设每秒钟

5K请求->一秒内支持2K请求的系统->系统崩溃

**使用MQ进行削峰**

5K请求->写入MQ->系统每秒从MQ拉取2K条消息进行消费->MySQL落库

#### 优缺点	

##### 优点

解耦 异步 削峰

##### 缺点

**系统可用性降低**

MQ一挂整套系统崩溃

**系统复杂度提高**

加个MQ进来，会导致可能出现的问题：重复消费，消息丢失，消息传递的顺序性

**一致性问题**

之前异步的处理方案，BC系统消费完写库成功，D系统失败，这就导致了一致性问题。

#### 技术选型

**中小型公司**技术实力较为一般，项目技术挑战不是特别高，用RabbitMQ是不错的选择

**大型公司**基础架构研发实力较强，用RocketMQ是很好的选择。

**大型数据领域**的实时计算,日志采集系统，用Kafka是业内标准的，几乎是全世界这个领域的事实性规范。

### 如何保证消息队列的高可用

#### RabbitMQ

基于主从（非分布式）做高可用性

##### 单机模式（无高可用）

demo级别，没人在生产用单机模式。

##### 普通集群模式（无高可用）

没做到分布式，就是个普通集群，没有高可用性，这个方案能提高吞吐量。

##### 镜像集群模式

元数据和queue里的消息都会存在于多个RabbitMQ节点上。

缺点：

1.性能开销大，消息需要同步到所有机器上，导致网络带宽压力和消耗很重。

2.不是分布式的，没有扩展性可言。

#### Kafka

Kafka架构->多个broker组成->每个broker是一个节点(一台服务器)

topic->划分多个partition->每个partition存放一部分数据->存放于一个broker上

**天然的分布式消息队列**

一个topic分散存放在多个机器上。

##### 高可用(HA机制)

每个partition->有多个replica副本在不同的机器上->所有replica选举出一个leader（生产和消费都跟这个leader打交道)->其他replica就是follower

**为何只能读写leader?**

要是可以允许随意读写每一个follower，就会产生数据一致性的问题，系统复杂度太高。

### 如何保证消息不被重复消费

本质上是

**使用消息队列如何保证幂等性**

都得结合业务思考，举几个例子：

1.有一条消息要写库，先根据主键查询，如果这数据有了，就直接丢弃或者更新。

2.写redis，每次都是set，天然幂等性

3.基于数据库的主键约束来避免重复消费

### 如何保证消息的可靠性传输

##### RabbitMQ

###### 生产者

生产者向RabbitMQ发送数据的时候，因为网络问题丢失了数据。

**解决方案**

生产者开启异步confirm模式->消息先落库->发送消息->RabbitMQ会给你回传一个ack消息->将落库的消息打标记为已经发送成功

定时任务->抓取未发送状态并且创建时间大于一定范围的消息->重新进行消息发送->超过3次未发送成功则标记发送失败

###### RabbitMQ

消息存在于内存中，还没有消费掉，RabbitMQ挂掉了导致消息丢失。

**解决方案**

**开启RabbitMQ的持久化**

1.创建queue的时候将其设置为持久化，这样就可以保证持久化queue的元数据。

2.发送消息的时候将消息的deliveryMode(投递模式)设置为2，也就是将消息设置为持久化的，此时RabbitMQ就会将消息持久化到磁盘上。

**优化点**

持久化跟生产者的confirm机制配合起来，只有消息被持久化到磁盘后，才会通知生产者ack了。

###### 消费者

刚消费到，还没处理，结果进程挂了，比如重启。

**解决方案**

**关闭RabbitMQ的自动ack**，在代码中消费完消息，处理完业务，再去ack一把。

##### Kafka

###### 生产者

按照下面第3,4点设置，一定不会丢。

###### Kafka

某个broker(leader)宕机=>其他follower还未同步数据=>重新选举leader=>消息数据丢失

**解决方案**

1. topic设置 replication.factor>1，要求每个partition至少有2个副本。

2. 服务端设置min.insync.replicas>1，要求leader感知至少一个follower跟自己保持联系。

3. producer端设置acks=all，要求每条数据写入所有replica之后，才能认为是写成功了。

4. producer端设置retries=MAX(很大的值)，一旦写入失败，无限重试。

###### 消费者

刚消费这个消息，还没处理完，**自动提交了offset**，然后挂掉了。（同RabbitMQ差不多）

**解决方案**

**关闭自动提交offset**

### 如何保证消息的顺序性

在一些场景下，需要保证传入消息队列的消息按顺序消费。

#### RabbitMQ

##### 错乱场景

一个queue=>生产者发送三条数据data1，data2，data3(期望消费顺序1，2,3)=>三个消费者消费顺序data2，data1，data3

##### 解决方案

拆分多个queue=>每个queue对应一个消费者=>data1，data2，data3按顺序传入同一个队列=>消费者内部可以采用单线程消费

#### Kafka

##### 错乱场景

topic=>三个partition=>指定一个key发送相关数据=>同一个partition=>数据有序=>消费者使用多线程消费=>乱序

##### 解决方案

1.

一个topic=>一个partition=>一个consumer=>内部单线程消费(吞吐量低)

2.

一个topic=>一个partition=>一个consumer=>内部多个内存队列=>相同key的数据存到同一个内存queue=>每个线程分别消费一个内存queue

### 消息积压相关

现在消费端出故障了，然后大量消息在 mq 里积压，现在出事故了。

#### 几百万消息持续积压几小时

1. 修复consumer，恢复消费速度，然后将现有consumer都停掉
2. 新建topic,partition是原来的10倍 = 临时建立好原先10倍的queue数量
3. 写一个临时分发的consumer程序=>将原来积压的大量消息不做耗时消费=>均匀轮询写入到临时建立的10倍数量的队列
4. 10倍机器来部署consumer=>每一台设备对应的consumer消费一个临时queue的数据
5. 消费完积压数据，恢复原先部署的架构，重新用原先的consumer机器来消费消息。

#### 如何解决消息队列的延时以及过期失效问题

场景：

RabbtMQ积压了大量TTL消息=>消息过期=>直接搞丢

解决：

批量重导=>手动写程序将丢失的消息全部查询出来=>重新写入MQ

#### 消息队列满了以后该怎么处理

临时写程序=>消费一个丢弃一个=>快速消费掉所有消息=>凌晨来批量重导

### 如何设计一个消息队列

设计思路，考虑以下几点

1. 可伸缩性=>需要时快速扩容=>增加吞吐量和容量=>参照Kafka的设计理念
2. 持久化=>落地磁盘=>是否需要顺序写=>这样性能高
3. 可用性  
4. 支持数据零丢失

## 搜索引擎

### ES 的分布式架构原理是什么

1. 分布式搜索引擎=>底层基于lucene=>多台机器上启动多个ES进程实例=>组成ES集群
2. 存储数据的基本单位是索引 类比数据库可以这样
   index=>表
   document=>行
   field=>列
   mapping=>表结构定义
   node=>每一个服务器
   shard replica=>数据分片与备份
3. index=>拆分多个shard=>支持横向扩展=>提高性能(吞吐量)
4. shard=>由primary shard 和 多个replica shard组成=>primary shard写入数据=>同步到其他replica shard
5. 多个node=>选举一个master节点=>管理工作(维护索引元数据 切换primary shard和 replica shard)
   master节点宕机=>重新选举一个节点为master节点
6. 非master节点宕机了=>此节点上的primary shard失效了=>master节点会让对应的replica shard切换为primary shard
   宕机的节点修复了=>原来的primary shard=>成为replica shard

### ES 写入数据的工作原理是什么

#### es写数据过程

客户端选择一个node(协调节点)=>节点对document进行路由=>请求转发给对应的node(primary shard)=>处理请求然后同步到 replica node=>协调节点返回响应结果给客户端。

#### es读数据过程

客户端发送请求到任意一个node(协调节点)=>对 doc id 进行哈希路由=>请求转发到对应的node=>使用随机轮询算法在primary和replica shard中随机选择一个=>
接受请求的node返回document给协调节点=>协调节点返回document给客户端

#### es搜索数据过程

客户端发送请求到协调节点=>转发到所有shard(primary 或者 replica都可)=>每个shard将自己的搜索结果( doc id)返回给协调节点=>协调节点进行数据的合并，排序，分页=>协调节点根据 doc id 去各个节点拉取实际的document数据=>返回给客户端

#### 写数据底层原理

数据写入buffer=>每隔一秒refresh到 os cache(数据能被搜索到)=>每隔5s将数据写入translog文件(最多5s数据丢失)=>translog大到一定程度或者每隔30min=>

触发commit操作将缓冲区的数据flush到segment file磁盘文件中(倒排索引已建立)

#### 删除/更新数据底层原理

**删除**

commit的时候生成一个.del文件=>将某个doc标识为deleted状态=>搜索的时候根据.del文件判断这个doc是否被删除

**更新**

原来的doc标识为deleted=>新写入一条数据

**物理上的删除**
定期执行merge将多个segment合并成一个=>标识为deleted的doc给物理删除掉

#### 底层lucene

jar包，封装好各种建立倒排索引的算法代码，lucene将已有的数据建立索引，在本地磁盘上组织它的数据结构。

#### 倒排索引

倒排索引就是关键词到文档ID的映射。

1. 所有词项对应一个或多个文档
2. 词项根据字典顺序升序排列

### ES 在数据量很大的情况下（数十亿级别）如何提高查询效率啊？

#### 文件系统缓存

1. filesystem cache 足够大尽可能容纳所有的idx segment file(索引数据文件)
2. es中存储数据，尽量只存需要用来检索的字段
3. es+mysql/hbase（es中检索得到数据条目的id,再通过id去mysql中查询所有信息）

#### 数据预热

对于比较热的，经常会有人访问的数据，做一个专门的**缓存预热子系统**。

系统的功能是对热数据每隔一段时间，提前访问一下，让数据进入filesystem cache 。

#### 冷热分离

冷数据写入一个索引，热数据写入另一个索引中，确保热数据在被预热之后，尽量让他们留在filesystem os cache里，不让冷数据给冲刷掉。

#### document模型设计

es中避免关联查询，而是将关联好的数据直接放入es。

#### 分页性能优化

1. 不允许深度分页
2. 滚动分页(scroll api)

### ES 生产集群的部署架构是什么？

1. es生产集群我们部署了5台机器，每台机器是6核64G，集群总内存是320G。
2. 集群日增量数据大概2000万条，日新增占存储空间500MB，每月是6亿条数据，新增占用空间15G，运行几个月，数据总量大概100G左右。
3. 线上有五个索引，每个索引的数据量大概是20G，每个索引分配的是8个shard，比默认的5个shard多了3个shard。

## 缓存

### 项目中缓存是如何使用的

#### 为什么要用缓存

##### 高性能

对于复杂操作耗时查出来的结果，确定后面不怎么变化，但是有很多读请求，直接将查询出来的结果放入缓存，后面直接读缓存就好。

##### 高并发

mysql单机2000qps容易报警。

缓存支持几万到几十万qps不是问题，内存天然支持高并发。

#### 用了缓存之后有什么不良后果

1. 缓存与数据库双写不一致
2. 缓存雪崩，缓存穿透，缓存击穿
3. 缓存并发竞争

### Redis 和 Memcached 有什么区别

#### redis和memcached区别

##### redis支持复杂的数据结构

##### redis原生支持集群模式

##### 性能对比

由于redis使用单核，memcached使用多核。存储100k以上的数据，memcached性能要高于redis。

#### redis线程模型

单线程的文件事件处理器，结构如下

1. 多个socket
2. IO多路复用程序
3. 文件事件分派器
4. 事件处理器(连接应答处理器，命令请求处理器，命令回复处理器)

#### 单线程效率高

1. 纯内存操作
2. 核心是基于非阻塞的IO多路复用机制
3. C语言实现

#### redis6.0开始引入多线程

redis的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程。

### Redis有哪些数据类型以及适用场景

#### Strings

最简单的key-value

![](img\1.png)

适合做简单的KV缓存。

#### Hashes

类似于map

![](img\2.png)

适合存储一个没有嵌套其他对象的对象。

#### Lists

有序列表。

![](img\3.png)

适用场景：存储粉丝列表，文章的评论列表等。高性能分页可以做微博那种不断下拉功能。

#### Sets

无序集合，自动去重。

![](img\4.png)

应用场景：交集，并集，差集。例如，粉丝列表查看共同好友。

#### Sorted Sets

 可以去重，可以排序的集合。

![](img\5.png)

### Redis的过期策略都有哪些

#### 过期策略

##### 定期删除

默认每隔100ms**随机抽取一些**设置过期时间的key，检查其是否过期，如果过期就删除。

##### 惰性删除

获取Key的时候，如果此时key已经过期，就删除，不会返回任何东西。

#### 内存淘汰机制

redis的key太多了，内存不够用，就要走内存淘汰机制。以下是比较常用的几个机制。

**allkeys-lru**：当内存不足以容纳新写入数据时，在**键空间**中，移除最近最少使用的 key（这个是**最常用**的）

**volatile-random:** 当内存不足以容纳新写入数据时，在**设置了过期时间的键空间**中，**随机移除**某个 key。

**volatile-ttl:** 当内存不足以容纳新写入数据时，在**设置了过期时间的键空间**中，有**更早过期时间**的 key 优先移除。